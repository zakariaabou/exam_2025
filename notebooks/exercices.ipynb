{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préliminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/zakariaabou/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "outputId": "2133f68b-4a2d-4fde-b043-a4e65ef09be6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exam_2025'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 59 (delta 21), reused 20 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (59/59), 1.41 MiB | 16.37 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie théorique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit être personnel). Cet entier servira de graine au générateur de nombres aléatoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 333"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation linéaire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef 'inputs' donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle méthode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming train_set and test_set are defined as in the provided code.\n",
        "\n",
        "# Extract data from the training set\n",
        "X_train = torch.tensor(train_set['inputs'], dtype=torch.float32)\n",
        "y_train = torch.tensor(train_set['targets'], dtype=torch.float32).reshape(-1, 1)  # Reshape to column vector\n",
        "\n",
        "# Add a column of ones for the intercept term (theta_0)\n",
        "X_train = torch.cat((torch.ones(X_train.shape[0], 1), X_train), dim=1)\n",
        "\n",
        "\n",
        "# Calculate the coefficients using the normal equation (least squares)\n",
        "theta_hat = torch.linalg.lstsq(X_train, y_train).solution\n",
        "\n",
        "# Print the estimated coefficients\n",
        "print(\"Estimated coefficients (theta_hat):\\n\", theta_hat)"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn",
        "outputId": "3eada9af-6339-4151-d0c6-187dda8b73ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated coefficients (theta_hat):\n",
            " tensor([[16.5580],\n",
            "        [ 3.2730],\n",
            "        [ 3.2948],\n",
            "        [ 6.6870]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXGXg8tlPULY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD. Quelle architecture s'y prête ? Justifier en termes d'expressivité et de performances en généralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "# Dataset et dataloader :\n",
        "dataset = Dataset1(train_set['inputs'], train_set['targets'])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "# A coder :\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc = nn.Linear(3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entraîner cette architecture à la tâche de régression définie par les entrées et sorties du jeu d'entraînement (compléter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "        optimizer.zero_grad()  # Réinitialisation des gradients\n",
        "        outputs = mySimpleNet(batch_inputs)  # Prédictions du modèle\n",
        "        loss = criterion(outputs, batch_targets)  # Calcul de la perte\n",
        "        loss.backward()  # Calcul des gradients\n",
        "        optimizer.step()  # Mise à jour des paramètres du modèle\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-",
        "outputId": "13e55365-428f-4db6-ca97-406168ff7343",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 300.1123962402344\n",
            "Epoch 10, Loss: 58.33413314819336\n",
            "Epoch 20, Loss: 36.03379821777344\n",
            "Epoch 30, Loss: 46.22721481323242\n",
            "Epoch 40, Loss: 29.654056549072266\n",
            "Epoch 50, Loss: 33.96439743041992\n",
            "Epoch 60, Loss: 37.58482360839844\n",
            "Epoch 70, Loss: 24.910655975341797\n",
            "Epoch 80, Loss: 28.84027099609375\n",
            "Epoch 90, Loss: 28.49748420715332\n",
            "Epoch 100, Loss: 41.96553421020508\n",
            "Epoch 110, Loss: 31.527070999145508\n",
            "Epoch 120, Loss: 40.82662582397461\n",
            "Epoch 130, Loss: 45.983551025390625\n",
            "Epoch 140, Loss: 27.101842880249023\n",
            "Epoch 150, Loss: 34.97255325317383\n",
            "Epoch 160, Loss: 28.91143035888672\n",
            "Epoch 170, Loss: 33.78229522705078\n",
            "Epoch 180, Loss: 37.98525619506836\n",
            "Epoch 190, Loss: 37.588417053222656\n",
            "Epoch 200, Loss: 33.54921340942383\n",
            "Epoch 210, Loss: 27.253435134887695\n",
            "Epoch 220, Loss: 34.91481018066406\n",
            "Epoch 230, Loss: 43.703548431396484\n",
            "Epoch 240, Loss: 41.667137145996094\n",
            "Epoch 250, Loss: 36.991493225097656\n",
            "Epoch 260, Loss: 34.33574676513672\n",
            "Epoch 270, Loss: 40.04707336425781\n",
            "Epoch 280, Loss: 38.69017028808594\n",
            "Epoch 290, Loss: 31.864030838012695\n",
            "Epoch 300, Loss: 41.64060592651367\n",
            "Epoch 310, Loss: 35.525840759277344\n",
            "Epoch 320, Loss: 34.60255813598633\n",
            "Epoch 330, Loss: 33.123165130615234\n",
            "Epoch 340, Loss: 31.962425231933594\n",
            "Epoch 350, Loss: 48.61721420288086\n",
            "Epoch 360, Loss: 33.272708892822266\n",
            "Epoch 370, Loss: 39.610111236572266\n",
            "Epoch 380, Loss: 35.729408264160156\n",
            "Epoch 390, Loss: 35.43424606323242\n",
            "Epoch 400, Loss: 34.204872131347656\n",
            "Epoch 410, Loss: 33.94063186645508\n",
            "Epoch 420, Loss: 26.03768539428711\n",
            "Epoch 430, Loss: 41.63469696044922\n",
            "Epoch 440, Loss: 29.998619079589844\n",
            "Epoch 450, Loss: 37.78635025024414\n",
            "Epoch 460, Loss: 44.30941390991211\n",
            "Epoch 470, Loss: 32.191165924072266\n",
            "Epoch 480, Loss: 40.49197006225586\n",
            "Epoch 490, Loss: 38.792579650878906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Où sont alors stockées les estimations des  $\\theta_k$ ? Les extraire du réseau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraction des estimations des θk\n",
        "theta_estimated_bias = mySimpleNet.fc.bias.data\n",
        "theta_estimated_weights = mySimpleNet.fc.weight.data.flatten()\n",
        "\n",
        "# Combinaison des biais et des poids\n",
        "theta_estimated = np.concatenate((theta_estimated_bias, theta_estimated_weights))\n",
        "print(\"Estimations des θk:\", theta_estimated)"
      ],
      "metadata": {
        "id": "EjgWp1y1rseb",
        "outputId": "9addb72b-8613-41ef-aac3-0cb10e1c0c61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimations des θk: [19.918228    0.02555635  0.02251558  0.06640711]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test data\n",
        "X_test = torch.tensor(test_set['inputs'], dtype=torch.float32)\n",
        "y_test = torch.tensor(test_set['targets'], dtype=torch.float32).reshape(-1, 1)\n",
        "X_test = torch.cat((torch.ones(X_test.shape[0], 1), X_test), dim=1)\n",
        "\n",
        "# Predictions using the estimated coefficients from Q1 (least squares)\n",
        "y_pred_q1 = torch.matmul(X_test, theta_hat)\n",
        "\n",
        "# Predictions using the trained neural network (Q4)\n",
        "with torch.no_grad(): # Ensure no gradient computation during prediction\n",
        "    y_pred_q4 = mySimpleNet(torch.tensor(test_set['inputs'], dtype=torch.float32))\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for both methods\n",
        "mse_q1 = nn.MSELoss()(y_pred_q1, y_test)\n",
        "mse_q4 = nn.MSELoss()(y_pred_q4, y_test)\n",
        "\n",
        "# Combinaison des biais et des poids\n",
        "theta_estimated = np.concatenate((theta_estimated_bias.numpy(), theta_estimated_weights.numpy()))\n",
        "print(\"Estimations des θk du modèle (via réseau de neurones):\", theta_estimated)\n",
        "\n",
        "# Comparaison avec les estimations obtenues dans la question 1 (moindres carrés)\n",
        "# Supposons que vous avez déjà calculé theta_hat via l'équation normale dans la question 1\n",
        "\n",
        "# Imprimer les estimations obtenues par la méthode des moindres carrés\n",
        "print(\"Estimations des θk par méthode des moindres carrés :\", theta_hat.numpy())\n",
        "\n",
        "print(f\"Mean Squared Error (Q1 - Least Squares): {mse_q1.item()}\")\n",
        "print(f\"Mean Squared Error (Q4 - Neural Network): {mse_q4.item()}\")\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "if mse_q1 < mse_q4:\n",
        "    print(\"Least squares method (Q1) provides a better fit on the test set.\")\n",
        "elif mse_q1 > mse_q4:\n",
        "    print(\"Neural network method (Q4) provides a better fit on the test set.\")\n",
        "else:\n",
        "    print(\"Both methods provide similar fits on the test set.\")"
      ],
      "metadata": {
        "id": "XvRH4FeOiARm",
        "outputId": "4f76d0d3-b389-4f5f-9eba-18771bc8e0c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimations des θk du modèle (via réseau de neurones): [19.918228    0.02555635  0.02251558  0.06640711]\n",
            "Estimations des θk par méthode des moindres carrés : [[16.558023 ]\n",
            " [ 3.2729847]\n",
            " [ 3.2948358]\n",
            " [ 6.687037 ]]\n",
            "Mean Squared Error (Q1 - Least Squares): 3.993152618408203\n",
            "Mean Squared Error (Q4 - Neural Network): 32.80284881591797\n",
            "\n",
            "Comparison:\n",
            "Least squares method (Q1) provides a better fit on the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "VvV2jIrBNtzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ réceptif et prédiction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau défini dans la cellule suivante est utilisé pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une série temporelle d'entrée et la valeur présente $y_t$ d'une série temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Up_causal, Down_causal\n",
        "\n",
        "class Double_conv_causal(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2, with causal convolutions that preserve input size'''\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
        "        super(Double_conv_causal, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9",
        "outputId": "df5d682e-31df-4437-b8d9-399bf43a8948",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de réseau de neurones s'agit-il ? Combien de paramètres la couche self.Down1 compte-t-elle (à faire à la main) ?\n",
        "Combien de paramètres le réseau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C'est un réseau de neurones convolutifs causal.\n",
        "# Nb de paramètres dans self.Down1: (calcul \"à la main\")\n",
        "# Nombre de parametre = (Cin*taille_du_kernel + 1)*Cout (1 pour le biais)\n",
        "#Cin = 64\n",
        "#Cout = 128\n",
        "#taille_du_kernel = 3\n",
        "#(64×3+1)×128=24,704\n",
        "\n",
        "# Nb de paramètres au total:\n",
        "# Définir le modèle\n",
        "model = causalFCN()\n",
        "\n",
        "# Calculer le nombre total de paramètres\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Nombre total de paramètres : {total_params}\")\n"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1",
        "outputId": "b70b0c14-8742-46d3-db04-fa12416b5f30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de paramètres : 2872641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?"
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La taille du vecteur d'entrée est réduite par des couches de convolution causale et du pooling dans les couches Down_causal, notamment avec un stride de 5 dans down3. La taille est restituée par des convolutions transposées dans les couches Up_causal, qui augmentent la taille de la séquence tout en utilisant les informations des couches précédentes pour affiner la sortie."
      ],
      "metadata": {
        "id": "PZafM9NJnQV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Le champ réceptif est augmenté par l'utilisation de convolutions dilatées et du down-sampling.\n",
        "\n"
      ],
      "metadata": {
        "id": "KRLF8P0kn5Vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici le calcul détaillé du champ réceptif en sortie de `self.inc` :\n",
        "\n",
        "1. **Première convolution** :\n",
        "   - Taille du noyau \\( k = 3 \\)\n",
        "   - Dilatation \\( d = 1 \\)\n",
        "   Le champ réceptif \\( r1 \\) pour la première convolution est donné par la formule :\n",
        "\n",
        "   r1 = (k - 1)\\*d + 1 = (3 - 1)\\*1 + 1 = 3\n",
        "\n",
        "2. **Deuxième convolution** :\n",
        "   - Taille du noyau \\( k = 3 \\)\n",
        "   - Dilatation \\( d = 1 \\)\n",
        "   Le champ réceptif \\( r2 \\) pour la deuxième convolution est également de 3, donc :                \n",
        "   r2 = 3\n",
        "\n",
        "3. **Champ réceptif total** :\n",
        "   Comme les convolutions sont appliquées successivement, le champ réceptif total \\( rtotal \\) est la somme du champ réceptif de la première et de la seconde convolution, moins 1 :\n",
        "   rtotal = r1 + (r2 - 1) = 3 + (3 - 1) = 5\n",
        "\n",
        "####Résultat :\n",
        "Le champ réceptif en sortie de `self.inc` est de **5**."
      ],
      "metadata": {
        "id": "SQYwW2LzoMU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. (Indice: considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...)"
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'model' and 'input_tensor1' are defined as in the previous code.\n",
        "\n",
        "# Create two input tensors that differ only at index 0\n",
        "input_tensor2 = copy.deepcopy(input_tensor1)\n",
        "input_tensor2[0, 0, 0] = 1  # Change a single value in the input\n",
        "\n",
        "# Perform the forward pass for both inputs\n",
        "output1 = model(input_tensor1)\n",
        "output2 = model(input_tensor2)\n",
        "\n",
        "\n",
        "# Find the receptive field size\n",
        "diff = torch.abs(output1 - output2)\n",
        "receptive_field = torch.nonzero(diff.squeeze())\n",
        "if len(receptive_field) > 0:\n",
        "  print(\"Receptive field size (empirical):\", receptive_field.max().item() + 1)\n",
        "else:\n",
        "  print(\"Could not determine the receptive field.\")"
      ],
      "metadata": {
        "id": "69WMWCSZAg5_",
        "outputId": "5c29d075-f693-406f-e1c1-ae09a8b6b74d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Receptive field size (empirical): 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two input tensors that differ only after index 5000\n",
        "input_tensor3 = copy.deepcopy(input_tensor1)\n",
        "input_tensor3[0, 0, 5001:] = 1  # Change values after index 5000\n",
        "\n",
        "# Perform the forward pass for both inputs\n",
        "output3 = model(input_tensor3)\n",
        "\n",
        "# Compare the outputs at index 5000\n",
        "diff_at_5000 = torch.abs(output1[0, 0, 5000] - output3[0, 0, 5000])\n",
        "print(f\"Difference at index 5000: {diff_at_5000}\")\n",
        "\n",
        "# The difference should be zero if the network is truly causal, which means that changing the inputs after\n",
        "# index 5000 does not affect the output at index 5000.\n",
        "\n",
        "# The part of the code in `Double_conv_causal` that guarantees causality is the padding operation:\n",
        "#\n",
        "# x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "#\n",
        "# This padding adds zeros to the left side of the input tensor before the convolution is applied. The amount of padding\n",
        "# is determined by the kernel size and the dilation.  By padding the input in this manner, convolutions only use\n",
        "# information from the input that is temporally before the output element being calculated.\n",
        "# This ensures that the output at a specific index doesn't depend on future inputs."
      ],
      "metadata": {
        "id": "PeooRYE-ATGt",
        "outputId": "e4f63bc2-d923-4504-ca22-447dd89859b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference at index 5000: 1.227647304534912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article récent](https://https://arxiv.org/abs/2403.14144) revient sur les progrès en matière de learning to rank. En voilà un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG?token=GHSAT0AAAAAAC427DACOPGNDNN6UDOLVLLAZ4BB2JQ\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'où proviennent les $z_i$ ? Que représentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle à ce que, après apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les réseaux de neurones exploités et la modalité suivant laquelle ils sont entraînés ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}